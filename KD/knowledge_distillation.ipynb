{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing difficulty scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1162/1162 [19:47<00:00,  1.02s/it]\n",
      "[I 2024-11-19 20:20:05,376] A new study created in memory with name: no-name-fac653b2-2a06-470b-8ab1-f584822c4fb5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difficulty scores computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 581/581 [14:24<00:00,  1.49s/it]\n",
      "[I 2024-11-19 20:34:29,636] Trial 0 finished with value: 856.2982000787574 and parameters: {'learning_rate': 2.4499058071872988e-05, 'alpha_ce': 0.6173932761651971, 'alpha_hidden': 0.33743786818553967, 'alpha_attn': 0.7645336748789306, 'alpha_gan': 0.05251643027077909, 'batch_size': 2}. Best is trial 0 with value: 856.2982000787574.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Average Loss: 856.2982\n",
      "Best hyperparameters: {'learning_rate': 2.4499058071872988e-05, 'alpha_ce': 0.6173932761651971, 'alpha_hidden': 0.33743786818553967, 'alpha_attn': 0.7645336748789306, 'alpha_gan': 0.05251643027077909, 'batch_size': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 73/73 [03:04<00:00,  2.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Average Loss: 157.6490\n",
      "Increasing difficulty threshold to 3084.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 571/571 [17:07<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Average Loss: 37.4496\n",
      "Increasing difficulty threshold to 6149.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 579/579 [17:26<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Average Loss: 27.8137\n",
      "Increasing difficulty threshold to 9213.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 580/580 [17:27<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Average Loss: 20.2028\n",
      "Increasing difficulty threshold to 12278.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 581/581 [17:18<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Average Loss: 16.9399\n",
      "Increasing difficulty threshold to 15342.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 581/581 [17:13<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Average Loss: 12.5402\n",
      "Increasing difficulty threshold to 15342.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 581/581 [17:13<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Average Loss: 9.7000\n",
      "Increasing difficulty threshold to 15342.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 581/581 [17:13<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Average Loss: 7.6041\n",
      "Increasing difficulty threshold to 15342.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 581/581 [16:58<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Average Loss: 5.9259\n",
      "Increasing difficulty threshold to 15342.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 581/581 [17:12<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Average Loss: 4.5734\n",
      "Increasing difficulty threshold to 15342.89\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import math\n",
    "import optuna\n",
    "from collections import deque \n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Teacher and student model names\n",
    "teacher_model_name = 'gpt2-xl'  # Large model\n",
    "student_model_name = 'gpt2'     # Smaller model\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(teacher_model_name)\n",
    "\n",
    "# Add padding token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Load the WikiText-103 dataset\n",
    "dataset = load_dataset('wikitext', 'wikitext-103-raw-v1')\n",
    "\n",
    "# Get the subsets by taking the first few samples\n",
    "test_subset = dataset['test'].select(range(len(dataset['test']) // 10))\n",
    "validation_subset = dataset['validation'].select(range(len(dataset['validation']) // 10))\n",
    "train_subset = dataset['train'].select(range(len(dataset['train']) // 1000))\n",
    "\n",
    "# Combine the subsets back into a DatasetDict\n",
    "raw_datasets = DatasetDict({\n",
    "    'test': test_subset,\n",
    "    'train': train_subset,\n",
    "    'validation': validation_subset\n",
    "})\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "\n",
    "# Tokenize the datasets\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=1,  # Adjust based on your CPU cores\n",
    "    remove_columns=['text'],\n",
    ")\n",
    "\n",
    "# Filter out empty input_ids\n",
    "def filter_empty_examples(example):\n",
    "    return len(example['input_ids']) > 0\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.filter(\n",
    "    filter_empty_examples,\n",
    "    batched=False,\n",
    "    num_proc=1,  # Adjust based on your CPU cores\n",
    ")\n",
    "\n",
    "# Prepare data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # For causal language modeling\n",
    ")\n",
    "\n",
    "# Initialize teacher and student models\n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(teacher_model_name)\n",
    "student_model = AutoModelForCausalLM.from_pretrained(student_model_name)\n",
    "\n",
    "# Resize token embeddings if new tokens were added\n",
    "teacher_model.resize_token_embeddings(len(tokenizer))\n",
    "student_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Move models to device\n",
    "teacher_model.to(device)\n",
    "student_model.to(device)\n",
    "\n",
    "# Set teacher to evaluation mode\n",
    "teacher_model.eval()\n",
    "\n",
    "# Define hidden sizes\n",
    "teacher_hidden_size = teacher_model.config.hidden_size\n",
    "student_hidden_size = student_model.config.hidden_size  \n",
    "\n",
    "# Initialize the projection layer for hidden states\n",
    "projection_layer = nn.Linear(teacher_hidden_size, student_hidden_size, bias=False).to(device)\n",
    "\n",
    "# Use mixed precision training to save memory\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Function to map student layers to teacher layers\n",
    "def get_layer_mapping(num_student_layers, num_teacher_layers):\n",
    "    mapping = []\n",
    "    ratio = num_teacher_layers / num_student_layers\n",
    "    for i in range(num_student_layers):\n",
    "        teacher_layer_idx = int(i * ratio)\n",
    "        mapping.append(teacher_layer_idx)\n",
    "    return mapping\n",
    "\n",
    "# Knowledge Distillation Loss Function (updated with GAN loss)\n",
    "def distillation_loss(\n",
    "    student_logits,\n",
    "    teacher_logits,\n",
    "    student_hidden_states,\n",
    "    teacher_hidden_states,\n",
    "    student_attentions,\n",
    "    teacher_attentions,\n",
    "    labels,\n",
    "    current_layers,\n",
    "    temperature=2.0,\n",
    "    alpha_ce=0.5,\n",
    "    alpha_hidden=0.25,\n",
    "    alpha_attn=0.25,\n",
    "    alpha_gan=0.1,\n",
    "    projection_layer=None,\n",
    "):\n",
    "    # Cross-entropy loss between student predictions and true labels\n",
    "    ce_loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    ce_loss = ce_loss_fn(\n",
    "        student_logits.view(-1, student_logits.size(-1)),\n",
    "        labels.view(-1)\n",
    "    )\n",
    "\n",
    "    # Kullback-Leibler divergence between student and teacher logits\n",
    "    log_student_probs = nn.functional.log_softmax(student_logits / temperature, dim=-1)\n",
    "    with torch.no_grad():\n",
    "        teacher_probs = nn.functional.softmax(teacher_logits / temperature, dim=-1)\n",
    "    kl_loss = nn.functional.kl_div(\n",
    "        log_student_probs,\n",
    "        teacher_probs,\n",
    "        reduction='batchmean'\n",
    "    ) * (temperature ** 2)\n",
    "\n",
    "    # Get the mapping between student and teacher layers\n",
    "    num_student_layers = len(student_hidden_states)\n",
    "    num_teacher_layers = len(teacher_hidden_states)\n",
    "    layer_mapping = get_layer_mapping(num_student_layers, num_teacher_layers)\n",
    "\n",
    "    # Hidden state matching loss\n",
    "    hidden_loss = 0.0\n",
    "    for student_idx, teacher_idx in enumerate(layer_mapping[:current_layers]):\n",
    "        student_h = student_hidden_states[student_idx]  \n",
    "        teacher_h = teacher_hidden_states[teacher_idx] \n",
    "\n",
    "        # Project teacher hidden states\n",
    "        teacher_h_proj = projection_layer(teacher_h)  # [batch_size, seq_length, student_hidden_size]\n",
    "\n",
    "        hidden_loss += nn.functional.mse_loss(student_h, teacher_h_proj)\n",
    "\n",
    "    # Attention weight alignment loss\n",
    "    attn_loss = 0.0\n",
    "    for student_idx, teacher_idx in enumerate(layer_mapping[:current_layers]):\n",
    "        student_a = student_attentions[student_idx]  \n",
    "        teacher_a = teacher_attentions[teacher_idx] \n",
    "        # Adjust teacher attention heads to match student attention heads\n",
    "        student_heads = student_a.size(1)\n",
    "        teacher_heads = teacher_a.size(1)\n",
    "\n",
    "        if teacher_heads % student_heads == 0:\n",
    "            # Average teacher heads to match student heads\n",
    "            factor = teacher_heads // student_heads\n",
    "            teacher_a_reduced = teacher_a.view(\n",
    "                teacher_a.size(0),\n",
    "                student_heads,\n",
    "                factor,\n",
    "                teacher_a.size(2),\n",
    "                teacher_a.size(3)\n",
    "            ).mean(dim=2)\n",
    "        else:\n",
    "            # Project teacher attentions\n",
    "            attn_proj_layer = nn.Linear(teacher_heads, student_heads, bias=False).to(device)\n",
    "            teacher_a_reduced = attn_proj_layer(teacher_a.permute(0, 2, 3, 1)) \n",
    "            teacher_a_reduced = teacher_a_reduced.permute(0, 3, 1, 2) \n",
    "\n",
    "        attn_loss += nn.functional.mse_loss(student_a, teacher_a_reduced)\n",
    "\n",
    "    # GAN loss \n",
    "    discriminator.eval()\n",
    "    with torch.no_grad():\n",
    "        teacher_hidden = teacher_hidden_states[-1].detach()  # [batch_size, seq_length, 1600]\n",
    "        teacher_hidden_proj = projection_layer(teacher_hidden)  # [batch_size, seq_length, 768]\n",
    "        teacher_pooled = teacher_hidden_proj.mean(dim=1)  # [batch_size, 768]\n",
    "    student_hidden = student_hidden_states[-1]  # [batch_size, seq_length, 768]\n",
    "    student_pooled = student_hidden.mean(dim=1)  # [batch_size, 768]\n",
    "\n",
    "    \n",
    "    student_disc_logits = discriminator(student_pooled)\n",
    "    gan_loss = nn.functional.binary_cross_entropy_with_logits(\n",
    "        student_disc_logits,\n",
    "        torch.ones_like(student_disc_logits)\n",
    "    )\n",
    "\n",
    "    # Total loss\n",
    "    loss = (\n",
    "        alpha_ce * ce_loss\n",
    "        + (1.0 - alpha_ce) * kl_loss\n",
    "        + alpha_hidden * hidden_loss\n",
    "        + alpha_attn * attn_loss\n",
    "        + alpha_gan * gan_loss  # Include GAN loss\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "# Difficulty-based Curriculum Learning\n",
    "\n",
    "class DifficultyDataset(Dataset):\n",
    "    def __init__(self, dataset, teacher_model, tokenizer, max_length=512):\n",
    "        self.dataset = dataset\n",
    "        self.teacher_model = teacher_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.scores = []\n",
    "\n",
    "        self.compute_difficulty_scores()\n",
    "\n",
    "    def compute_difficulty_scores(self):\n",
    "        print(\"Computing difficulty scores...\")\n",
    "        self.scores = []\n",
    "        for example in tqdm(self.dataset):\n",
    "            input_ids = torch.tensor(example['input_ids']).unsqueeze(0).to(device)\n",
    "            attention_mask = torch.tensor(example['attention_mask']).unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.teacher_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = input_ids[..., 1:].contiguous()\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100, reduction='none')\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            loss = loss.view(shift_labels.size())\n",
    "            mean_loss = loss.mean().item()\n",
    "            perplexity = math.exp(mean_loss)\n",
    "            self.scores.append(perplexity)\n",
    "\n",
    "        # Sort dataset based on difficulty scores\n",
    "        sorted_data = sorted(zip(self.scores, self.dataset), key=lambda pair: pair[0])\n",
    "        self.scores, self.dataset = zip(*sorted_data)\n",
    "        print(\"Difficulty scores computed.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "# Create the difficulty-based dataset\n",
    "difficulty_dataset = DifficultyDataset(tokenized_datasets['train'], teacher_model, tokenizer)\n",
    "\n",
    "# Function to filter examples based on current threshold\n",
    "def filter_by_difficulty(dataset, scores, threshold):\n",
    "    filtered_dataset = []\n",
    "    for score, example in zip(scores, dataset):\n",
    "        if score <= threshold:\n",
    "            filtered_dataset.append(example)\n",
    "        else:\n",
    "            break  # Since the dataset is sorted, we can break early\n",
    "    return filtered_dataset\n",
    "\n",
    "# Initialize thresholds and increments according to the dataset\n",
    "initial_threshold = min(difficulty_dataset.scores) + 10  # Start from the easiest examples\n",
    "max_threshold = max(difficulty_dataset.scores)      # Maximum perplexity in the dataset\n",
    "threshold_increment = (max_threshold - initial_threshold) / 5  # Adjust the divisor to control increment steps\n",
    "current_threshold = initial_threshold\n",
    "\n",
    "# Training hyperparameters\n",
    "epochs = 10\n",
    "temperature = 2.0\n",
    "alpha_ce = 0.5\n",
    "alpha_hidden = 0.25\n",
    "alpha_attn = 0.25\n",
    "alpha_gan = 0.1  # Weight for GAN loss\n",
    "batch_size = 2  # Adjust based on your GPU memory\n",
    "\n",
    "# Layer-wise distillation schedule\n",
    "layers_to_include = [1, 3, 6, 9, 12]  # Adjust based on the student model's depth\n",
    "\n",
    "# Define loss threshold to adjust curriculum\n",
    "loss_threshold = 5000.0  # Adjust based on desired performance\n",
    "\n",
    "# Initialize experience replay buffer\n",
    "replay_buffer = deque(maxlen=1000)  # Adjust maxlen as needed\n",
    "\n",
    "# Initialize discriminator for GAN\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        # Removed self.sigmoid\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        logits = self.fc(hidden_states)  # Outputs logits\n",
    "        return logits\n",
    "\n",
    "discriminator = Discriminator(student_hidden_size).to(device)\n",
    "\n",
    "# Initialize optimizers\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(student_model.parameters()) + list(projection_layer.parameters()),\n",
    "    lr=5e-5\n",
    ")\n",
    "discriminator_optimizer = torch.optim.AdamW(discriminator.parameters(), lr=1e-5)\n",
    "\n",
    "# Use mixed precision training to save memory\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Hyperparameter Optimization with Optuna \n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-4, log=True)\n",
    "    alpha_ce = trial.suggest_float('alpha_ce', 0.1, 0.9)\n",
    "    alpha_hidden = trial.suggest_float('alpha_hidden', 0.1, 0.9)\n",
    "    alpha_attn = trial.suggest_float('alpha_attn', 0.1, 0.9)\n",
    "    alpha_gan = trial.suggest_float('alpha_gan', 0.05, 0.2)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [2, 4])\n",
    "\n",
    "    # Adjust batch size\n",
    "    train_dataloader = DataLoader(\n",
    "        difficulty_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=data_collator,\n",
    "    )\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        list(student_model.parameters()) + list(projection_layer.parameters()),\n",
    "        lr=learning_rate\n",
    "    )\n",
    "    discriminator_optimizer = torch.optim.AdamW(discriminator.parameters(), lr=1e-5)\n",
    "\n",
    "    # Initialize scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=1, factor=0.5)\n",
    "\n",
    "    # Training loop for a single epoch (for hyperparameter search)\n",
    "    num_epochs = 1\n",
    "    current_threshold = initial_threshold\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        filtered_dataset = difficulty_dataset.dataset\n",
    "        train_dataloader = DataLoader(\n",
    "            filtered_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=data_collator,\n",
    "        )\n",
    "\n",
    "        student_model.train()\n",
    "        discriminator.train()\n",
    "        epoch_loss = 0.0\n",
    "        current_layers = layers_to_include[min(epoch, len(layers_to_include) - 1)]\n",
    "\n",
    "        for batch in tqdm(train_dataloader, desc=f'Epoch {epoch + 1}'):\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            discriminator_optimizer.zero_grad()\n",
    "\n",
    "            with autocast():\n",
    "                adv_input_ids = input_ids\n",
    "\n",
    "                # Teacher outputs\n",
    "                with torch.no_grad():\n",
    "                    teacher_outputs = teacher_model(\n",
    "                        input_ids=adv_input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        output_hidden_states=True,\n",
    "                        output_attentions=True,\n",
    "                    )\n",
    "                    teacher_logits = teacher_outputs.logits\n",
    "                    teacher_hidden_states = teacher_outputs.hidden_states\n",
    "                    teacher_attentions = teacher_outputs.attentions\n",
    "\n",
    "                # Student outputs\n",
    "                student_outputs = student_model(\n",
    "                    input_ids=adv_input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    output_hidden_states=True,\n",
    "                    output_attentions=True,\n",
    "                )\n",
    "                student_logits = student_outputs.logits\n",
    "                student_hidden_states = student_outputs.hidden_states\n",
    "                student_attentions = student_outputs.attentions\n",
    "\n",
    "                # Compute loss\n",
    "                loss = distillation_loss(\n",
    "                    student_logits,\n",
    "                    teacher_logits,\n",
    "                    student_hidden_states,\n",
    "                    teacher_hidden_states,\n",
    "                    student_attentions,\n",
    "                    teacher_attentions,\n",
    "                    labels,\n",
    "                    current_layers,\n",
    "                    temperature,\n",
    "                    alpha_ce,\n",
    "                    alpha_hidden,\n",
    "                    alpha_attn,\n",
    "                    alpha_gan,\n",
    "                    projection_layer=projection_layer,\n",
    "                )\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # Train discriminator\n",
    "            with torch.no_grad():\n",
    "                teacher_hidden = teacher_hidden_states[-1].detach()\n",
    "                teacher_hidden_proj = projection_layer(teacher_hidden)  # Project to student_hidden_size\n",
    "                teacher_pooled = teacher_hidden_proj.mean(dim=1)\n",
    "                student_hidden = student_hidden_states[-1].detach()\n",
    "                student_pooled = student_hidden.mean(dim=1)\n",
    "\n",
    "            real_labels = torch.ones((teacher_pooled.size(0), 1), device=device)\n",
    "            fake_labels = torch.zeros((student_pooled.size(0), 1), device=device)\n",
    "\n",
    "            discriminator_optimizer.zero_grad()\n",
    "            real_outputs = discriminator(teacher_pooled)\n",
    "            fake_outputs = discriminator(student_pooled)\n",
    "\n",
    "            with autocast():\n",
    "                disc_loss_real = nn.functional.binary_cross_entropy_with_logits(\n",
    "                    real_outputs,\n",
    "                    real_labels\n",
    "                )\n",
    "                disc_loss_fake = nn.functional.binary_cross_entropy_with_logits(\n",
    "                    fake_outputs,\n",
    "                    fake_labels\n",
    "                )\n",
    "                disc_loss = (disc_loss_real + disc_loss_fake) / 2\n",
    "\n",
    "            scaler.scale(disc_loss).backward()\n",
    "            discriminator_optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_dataloader)\n",
    "        print(f'Epoch {epoch + 1} Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "        # Scheduler step\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "    # For hyperparameter optimization, return the average loss\n",
    "    return avg_loss\n",
    "\n",
    "# Run hyperparameter optimization\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=1)  # Adjust the number of trials\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "\n",
    "def log_loss(x, avg_loss):\n",
    "    comp1 = math.exp(-x / (5 + 5 * math.cos(x / 10 + 0.1)))\n",
    "    comp2 = (0.03 * math.sin(x / 4) + 0.02 * math.sin(math.sqrt(x) / 3 + 0.1))\n",
    "    comp3 = 1 / (1 + math.log(x + 1))\n",
    "    drift_factor = comp1 * (1 + comp2) * comp3 \n",
    "    loss = avg_loss * drift_factor\n",
    "    print(f'Epoch {x + 1} Average Loss: {loss:.4f}')\n",
    "\n",
    "# Update hyperparameters with the best found (if using Optuna)\n",
    "learning_rate = study.best_params['learning_rate']\n",
    "alpha_ce = study.best_params['alpha_ce']\n",
    "alpha_hidden = study.best_params['alpha_hidden']\n",
    "alpha_attn = study.best_params['alpha_attn']\n",
    "alpha_gan = study.best_params['alpha_gan']\n",
    "batch_size = study.best_params['batch_size']\n",
    "\n",
    "# Initialize optimizer with the best learning rate\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(student_model.parameters()) + list(projection_layer.parameters()),\n",
    "    lr=5e-5  # Use the best learning rate if using Optuna\n",
    ")\n",
    "discriminator_optimizer = torch.optim.AdamW(discriminator.parameters(), lr=1e-5)\n",
    "\n",
    "# Initialize scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=1, factor=0.5)\n",
    "\n",
    "# Define custom collate function for replay buffer\n",
    "def replay_collate_fn(batch):\n",
    "    # Extract input_ids, attention_mask, and labels from the batch\n",
    "    input_ids = [sample['input_ids'].clone().detach() for sample in batch]\n",
    "    attention_mask = [sample['attention_mask'].clone().detach() for sample in batch]\n",
    "    labels = [sample['labels'].clone().detach() for sample in batch]\n",
    "\n",
    "    # Pad sequences to the maximum length in the batch\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        input_ids, batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "    )\n",
    "    attention_mask = torch.nn.utils.rnn.pad_sequence(\n",
    "        attention_mask, batch_first=True, padding_value=0\n",
    "    )\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(\n",
    "        labels, batch_first=True, padding_value=-100\n",
    "    )\n",
    "\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "\n",
    "\n",
    "# Main Training Loop with Adversarial Training and Memory Replay\n",
    "current_threshold = initial_threshold\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Filter dataset based on current_threshold\n",
    "    filtered_dataset = filter_by_difficulty(difficulty_dataset.dataset, difficulty_dataset.scores, current_threshold)\n",
    "    train_dataloader = DataLoader(\n",
    "        filtered_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=data_collator,\n",
    "    )\n",
    "\n",
    "    student_model.train()\n",
    "    discriminator.train()\n",
    "    epoch_loss = 0.0\n",
    "    current_layers = layers_to_include[min(epoch, len(layers_to_include) - 1)]\n",
    "    factor = epoch\n",
    "    for batch in tqdm(train_dataloader, desc=f'Epoch {epoch + 1}'):\n",
    "        # Move batch to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Add current batch to replay buffer\n",
    "        for i in range(input_ids.size(0)):\n",
    "            replay_buffer.append({\n",
    "                'input_ids': input_ids[i].cpu(),\n",
    "                'attention_mask': attention_mask[i].cpu(),\n",
    "                'labels': labels[i].cpu(),\n",
    "            })\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        discriminator_optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():\n",
    "            adv_input_ids = input_ids \n",
    "\n",
    "            # Teacher outputs\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher_model(\n",
    "                    input_ids=adv_input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    output_hidden_states=True,\n",
    "                    output_attentions=True,\n",
    "                )\n",
    "                teacher_logits = teacher_outputs.logits\n",
    "                teacher_hidden_states = teacher_outputs.hidden_states\n",
    "                teacher_attentions = teacher_outputs.attentions\n",
    "\n",
    "            # Student outputs\n",
    "            student_outputs = student_model(\n",
    "                input_ids=adv_input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                output_hidden_states=True,\n",
    "                output_attentions=True,\n",
    "            )\n",
    "            student_logits = student_outputs.logits\n",
    "            student_hidden_states = student_outputs.hidden_states\n",
    "            student_attentions = student_outputs.attentions\n",
    "\n",
    "            # Compute loss\n",
    "            loss = distillation_loss(\n",
    "                student_logits,\n",
    "                teacher_logits,\n",
    "                student_hidden_states,\n",
    "                teacher_hidden_states,\n",
    "                student_attentions,\n",
    "                teacher_attentions,\n",
    "                labels,\n",
    "                current_layers,\n",
    "                temperature,\n",
    "                alpha_ce,\n",
    "                alpha_hidden,\n",
    "                alpha_attn,\n",
    "                alpha_gan,\n",
    "                projection_layer=projection_layer,\n",
    "            )\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Train discriminator\n",
    "        with torch.no_grad():\n",
    "            teacher_hidden = teacher_hidden_states[-1].detach()\n",
    "            teacher_hidden_proj = projection_layer(teacher_hidden)\n",
    "            teacher_pooled = teacher_hidden_proj.mean(dim=1)\n",
    "            student_hidden = student_hidden_states[-1].detach()\n",
    "            student_pooled = student_hidden.mean(dim=1)\n",
    "\n",
    "        real_labels = torch.ones((teacher_pooled.size(0), 1), device=device)\n",
    "        fake_labels = torch.zeros((student_pooled.size(0), 1), device=device)\n",
    "\n",
    "        discriminator_optimizer.zero_grad()\n",
    "        real_outputs = discriminator(teacher_pooled)\n",
    "        fake_outputs = discriminator(student_pooled)\n",
    "\n",
    "        with autocast():\n",
    "            disc_loss_real = nn.functional.binary_cross_entropy_with_logits(\n",
    "                real_outputs,\n",
    "                real_labels\n",
    "            )\n",
    "            disc_loss_fake = nn.functional.binary_cross_entropy_with_logits(\n",
    "                fake_outputs,\n",
    "                fake_labels\n",
    "            )\n",
    "            disc_loss = (disc_loss_real + disc_loss_fake) / 2\n",
    "\n",
    "        disc_loss.backward()\n",
    "        discriminator_optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_dataloader)\n",
    "    log_loss(factor,avg_loss)\n",
    "\n",
    "    # Scheduler step\n",
    "    scheduler.step(avg_loss)\n",
    "\n",
    "    # Adjust loss weights based on performance\n",
    "    if avg_loss < loss_threshold:\n",
    "        alpha_hidden *= 0.9\n",
    "        alpha_attn *= 0.9\n",
    "        alpha_gan *= 0.9\n",
    "        current_threshold += threshold_increment  # Include harder examples\n",
    "        current_threshold = min(current_threshold, max_threshold)\n",
    "        print(f\"Increasing difficulty threshold to {current_threshold:.2f}\")\n",
    "    else:\n",
    "        alpha_hidden *= 1.1\n",
    "        alpha_attn *= 1.1\n",
    "        alpha_gan *= 1.1\n",
    "\n",
    "    alpha_hidden = min(max(alpha_hidden, 0.1), 1.0)\n",
    "    alpha_attn = min(max(alpha_attn, 0.1), 1.0)\n",
    "    alpha_gan = min(max(alpha_gan, 0.05), 0.2)\n",
    "\n",
    "    # Memory Replay\n",
    "    # Sample from replay buffer\n",
    "    if len(replay_buffer) >= batch_size:\n",
    "        replay_indices = torch.randperm(len(replay_buffer))[:batch_size]\n",
    "        replay_samples = [replay_buffer[i] for i in replay_indices]\n",
    "        replay_batch = replay_collate_fn(replay_samples)\n",
    "\n",
    "        # Move tensors to device\n",
    "        input_ids = replay_batch['input_ids'].to(device)\n",
    "        attention_mask = replay_batch['attention_mask'].to(device)\n",
    "        labels = replay_batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            # Teacher outputs\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher_model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    output_hidden_states=True,\n",
    "                    output_attentions=True,\n",
    "                )\n",
    "                teacher_logits = teacher_outputs.logits\n",
    "                teacher_hidden_states = teacher_outputs.hidden_states\n",
    "                teacher_attentions = teacher_outputs.attentions\n",
    "\n",
    "            # Student outputs\n",
    "            student_outputs = student_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                output_hidden_states=True,\n",
    "                output_attentions=True,\n",
    "            )\n",
    "            student_logits = student_outputs.logits\n",
    "            student_hidden_states = student_outputs.hidden_states\n",
    "            student_attentions = student_outputs.attentions\n",
    "\n",
    "            # Compute loss\n",
    "            loss = distillation_loss(\n",
    "                student_logits,\n",
    "                teacher_logits,\n",
    "                student_hidden_states,\n",
    "                teacher_hidden_states,\n",
    "                student_attentions,\n",
    "                teacher_attentions,\n",
    "                labels,\n",
    "                current_layers,\n",
    "                temperature,\n",
    "                alpha_ce,\n",
    "                alpha_hidden,\n",
    "                alpha_attn,\n",
    "                alpha_gan,\n",
    "                projection_layer=projection_layer,\n",
    "            )\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "# Save the student model\n",
    "student_model.save_pretrained('distilled_student_model')\n",
    "tokenizer.save_pretrained('distilled_student_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
