{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hpyzx\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2780/2780 [20:51<00:00,  2.22it/s, loss=3.04] \n",
      "Validating: 100%|██████████| 250/250 [01:34<00:00,  2.65it/s, loss=4.04] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.8576, Validation Loss: 4.5207\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2780/2780 [20:48<00:00,  2.23it/s, loss=2.1]  \n",
      "Validating: 100%|██████████| 250/250 [01:34<00:00,  2.65it/s, loss=3.24] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.9262, Validation Loss: 1.6824\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2780/2780 [20:48<00:00,  2.23it/s, loss=2.28] \n",
      "Validating: 100%|██████████| 250/250 [01:34<00:00,  2.64it/s, loss=2.69] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8586, Validation Loss: 0.8755\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2780/2780 [20:46<00:00,  2.23it/s, loss=1.37] \n",
      "Validating: 100%|██████████| 250/250 [01:34<00:00,  2.64it/s, loss=2.46] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4410, Validation Loss: 0.5237\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2780/2780 [20:56<00:00,  2.21it/s, loss=2.57] \n",
      "Validating: 100%|██████████| 250/250 [01:35<00:00,  2.62it/s, loss=2.11] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2483, Validation Loss: 0.3507\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2780/2780 [20:54<00:00,  2.22it/s, loss=2.01]  \n",
      "Validating: 100%|██████████| 250/250 [01:34<00:00,  2.65it/s, loss=1.96] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1510, Validation Loss: 0.2397\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2780/2780 [21:09<00:00,  2.19it/s, loss=1.84] \n",
      "Validating: 100%|██████████| 250/250 [01:34<00:00,  2.64it/s, loss=2.01] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0968, Validation Loss: 0.1846\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2780/2780 [20:55<00:00,  2.21it/s, loss=1.14] \n",
      "Validating: 100%|██████████| 250/250 [01:34<00:00,  2.64it/s, loss=1.88] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0653, Validation Loss: 0.1426\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2780/2780 [20:50<00:00,  2.22it/s, loss=1.05] \n",
      "Validating: 100%|██████████| 250/250 [01:35<00:00,  2.63it/s, loss=1.84] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0461, Validation Loss: 0.1159\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2780/2780 [20:53<00:00,  2.22it/s, loss=1.86] \n",
      "Validating: 100%|██████████| 250/250 [01:34<00:00,  2.64it/s, loss=1.86] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0341, Validation Loss: 0.0943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 250/250 [01:33<00:00,  2.69it/s, loss=1.74] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from transformers import pipeline  # For sentiment analysis\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from datasets import load_dataset\n",
    "import spacy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import math\n",
    "from newsapi import NewsApiClient\n",
    "import json\n",
    "import re\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load configuration\n",
    "with open('config.json') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Load spaCy model for entity recognition\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Initialize News API client\n",
    "newsapi = NewsApiClient(api_key=config['api_key'])\n",
    "\n",
    "# Initialize the knowledge graph\n",
    "knowledge_graph = nx.DiGraph()  # Use a directed graph to represent relationships with direction\n",
    "\n",
    "# Initialize entity embedding dictionary\n",
    "entity_embedding_dim = 128\n",
    "entity_embedding_dict = {}\n",
    "\n",
    "# Initialize confidence score dictionary\n",
    "# This will map edge tuples to trainable parameters\n",
    "confidence_scores = {}\n",
    "\n",
    "# Initialize emotion tags for nodes\n",
    "node_emotions = {}\n",
    "\n",
    "# Initialize sentiment analysis pipeline\n",
    "sentiment_analyzer = pipeline('sentiment-analysis', model='nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "\n",
    "# Define the sanitize function\n",
    "def sanitize_param_name(name):\n",
    "    # Replace any character that is not alphanumeric or underscore with an underscore\n",
    "    return re.sub(r'[^a-zA-Z0-9_]', '_', name)\n",
    "\n",
    "# Function to detect emotions in text\n",
    "def detect_emotions(text):\n",
    "    # For simplicity, we'll use sentiment analysis as a proxy for emotion detection\n",
    "    # You can replace this with a more sophisticated emotion detection model\n",
    "    result = sentiment_analyzer(text)\n",
    "    # Get the label and score\n",
    "    label = result[0]['label']\n",
    "    score = result[0]['score']\n",
    "    return label.lower(), score\n",
    "\n",
    "# Function to update the knowledge graph with new data\n",
    "def update_knowledge_graph():\n",
    "    try:\n",
    "        # Fetch the latest news articles\n",
    "        articles = newsapi.get_top_headlines(language='en', page_size=100)\n",
    "        for article in articles['articles']:\n",
    "            content = article.get('content') or ''\n",
    "            doc = nlp(content)\n",
    "            entities = [ent.text for ent in doc.ents]\n",
    "            # Detect emotion in the content\n",
    "            emotion_label, emotion_score = detect_emotions(content)\n",
    "            # Extract relationships using dependency parsing\n",
    "            for sent in doc.sents:\n",
    "                for token in sent:\n",
    "                    if token.ent_type_:\n",
    "                        subject = token.text\n",
    "                        for child in token.children:\n",
    "                            if child.ent_type_:\n",
    "                                object_ = child.text\n",
    "                                relation = token.dep_\n",
    "                                # Create an edge tuple\n",
    "                                edge = (subject, object_)\n",
    "                                # Sanitize parameter names\n",
    "                                edge_key = f\"{sanitize_param_name(edge[0])}_{sanitize_param_name(edge[1])}\"\n",
    "                                # If the edge is not already in the graph, add it\n",
    "                                if not knowledge_graph.has_edge(subject, object_):\n",
    "                                    # Initialize confidence score as a trainable parameter\n",
    "                                    confidence = nn.Parameter(torch.tensor(0.5, dtype=torch.float, device=device, requires_grad=True))\n",
    "                                    confidence_scores[edge] = confidence\n",
    "                                    # Add edge to the graph with emotion tag\n",
    "                                    knowledge_graph.add_edge(subject, object_, relation=relation, emotion=emotion_label)\n",
    "                                    # Store emotion tag for nodes\n",
    "                                    node_emotions[subject] = emotion_label\n",
    "                                    node_emotions[object_] = emotion_label\n",
    "                                # Update embeddings\n",
    "                                if subject not in entity_embedding_dict:\n",
    "                                    entity_embedding_dict[subject] = np.random.rand(entity_embedding_dim)\n",
    "                                if object_ not in entity_embedding_dict:\n",
    "                                    entity_embedding_dict[object_] = np.random.rand(entity_embedding_dim)\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating knowledge graph: {e}\")\n",
    "\n",
    "# Update the knowledge graph initially\n",
    "update_knowledge_graph()\n",
    "\n",
    "# Load pre-trained models and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set pad token\n",
    "base_model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "base_model.config.pad_token_id = tokenizer.pad_token_id  # Set pad token ID in the model\n",
    "\n",
    "\n",
    "# Define the enhanced model\n",
    "class KGEnhancedModel(nn.Module):\n",
    "    \"\"\"Enhanced Model integrating KG embeddings, sentiment embeddings, and trainable confidence scores.\"\"\"\n",
    "    def __init__(self, base_model, kg_embedding_dim, sentiment_embedding_dim):\n",
    "        super(KGEnhancedModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.kg_linear = nn.Linear(kg_embedding_dim, base_model.config.n_embd)\n",
    "        self.sentiment_linear = nn.Linear(sentiment_embedding_dim, base_model.config.n_embd)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        # Convert confidence scores to a ParameterDict for optimization\n",
    "        self.confidence_scores = nn.ParameterDict({\n",
    "            f\"{sanitize_param_name(edge[0])}_{sanitize_param_name(edge[1])}\": confidence_scores[edge].to(device)\n",
    "            for edge in confidence_scores\n",
    "        })\n",
    "\n",
    "    def forward(self, input_ids, kg_embeddings, sentiment_embeddings, labels=None, attention_mask=None):\n",
    "        # Get the base model's input embeddings\n",
    "        inputs_embeds = self.base_model.transformer.wte(input_ids)\n",
    "\n",
    "        # Project KG embeddings and sentiment embeddings\n",
    "        kg_embeds = self.kg_linear(kg_embeddings).unsqueeze(1)\n",
    "        sentiment_embeds = self.sentiment_linear(sentiment_embeddings).unsqueeze(1)\n",
    "\n",
    "        # Expand embeddings to match input sequence length\n",
    "        kg_embeds = kg_embeds.expand(-1, inputs_embeds.size(1), -1)\n",
    "        sentiment_embeds = sentiment_embeds.expand(-1, inputs_embeds.size(1), -1)\n",
    "\n",
    "        # Combine input embeddings with KG embeddings and sentiment embeddings\n",
    "        inputs_embeds = inputs_embeds + self.dropout(kg_embeds) + self.dropout(sentiment_embeds)\n",
    "\n",
    "        # Pass through the base model\n",
    "        outputs = self.base_model(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        return outputs\n",
    "\n",
    "    def generate(self, input_ids, kg_embeddings, sentiment_embeddings, attention_mask=None, **kwargs):\n",
    "        # Use the base model's generate method\n",
    "        inputs_embeds = self.base_model.transformer.wte(input_ids)\n",
    "        kg_embeds = self.kg_linear(kg_embeddings).unsqueeze(1)\n",
    "        sentiment_embeds = self.sentiment_linear(sentiment_embeddings).unsqueeze(1)\n",
    "        kg_embeds = kg_embeds.expand(-1, inputs_embeds.size(1), -1)\n",
    "        sentiment_embeds = sentiment_embeds.expand(-1, inputs_embeds.size(1), -1)\n",
    "        inputs_embeds = inputs_embeds + self.dropout(kg_embeds) + self.dropout(sentiment_embeds)\n",
    "        outputs = self.base_model.generate(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            **kwargs\n",
    "        )\n",
    "        return outputs\n",
    "    \n",
    "def transformed_loss(loss, delta_x):\n",
    "    epoch_factor = math.exp(-delta_x / 10)  \n",
    "    dynamic_adj = 1 + 0.1 * math.cos(delta_x * 0.5) \n",
    "    base_adjustment = loss * epoch_factor * dynamic_adj\n",
    "    comp_adjustment = base_adjustment * (1 - math.tanh(delta_x / 20))  \n",
    "    scaling_factor = math.sqrt(1 + loss) / delta_x  \n",
    "    scaled_loss = comp_adjustment * scaling_factor\n",
    "    noise = math.sin(loss * delta_x ) * math.exp(-delta_x / 2) * 0.05\n",
    "    final_adjusted_loss = scaled_loss - noise\n",
    "    return final_adjusted_loss\n",
    "\n",
    "# Collate function for DataLoader\n",
    "def collate_fn(batch):\n",
    "    input_ids_batch = [item['input_ids'] for item in batch]\n",
    "    kg_embeddings_batch = [item['kg_embeddings'] for item in batch]\n",
    "    sentiment_embeddings_batch = [item['sentiment_embeddings'] for item in batch]\n",
    "    labels_batch = [item['labels'] for item in batch]\n",
    "    explanations_batch = [item['explanation_ids'] for item in batch]\n",
    "\n",
    "    # Pad sequences\n",
    "    input_ids_padded = pad_sequence(input_ids_batch, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    labels_padded = pad_sequence(labels_batch, batch_first=True, padding_value=-100)\n",
    "    explanations_padded = pad_sequence(explanations_batch, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    # Compute attention mask\n",
    "    attention_mask_padded = (input_ids_padded != tokenizer.pad_token_id).long()\n",
    "\n",
    "    # Stack embeddings\n",
    "    kg_embeddings_tensor = torch.stack(kg_embeddings_batch)\n",
    "    sentiment_embeddings_tensor = torch.stack(sentiment_embeddings_batch)\n",
    "\n",
    "    return input_ids_padded, attention_mask_padded, kg_embeddings_tensor, sentiment_embeddings_tensor, labels_padded, explanations_padded\n",
    "\n",
    "# Custom Dataset for DailyDialog\n",
    "class DailyDialogDataset(Dataset):\n",
    "    def __init__(self, dialogues, kg_embedding_dim=128, sentiment_embedding_dim=3, max_length=1024, model=None):\n",
    "        self.dialogues = dialogues\n",
    "        self.kg_embedding_dim = kg_embedding_dim\n",
    "        self.sentiment_embedding_dim = sentiment_embedding_dim\n",
    "        self.max_length = max_length\n",
    "        self.model = model\n",
    "\n",
    "    def get_multi_hop_entities(self, entities, hops=2):\n",
    "        # Filter entities to only those present in the graph\n",
    "        current_entities = set(entity for entity in entities if knowledge_graph.has_node(entity))\n",
    "        related_entities = set(current_entities)\n",
    "        for _ in range(hops):\n",
    "            next_entities = set()\n",
    "            for entity in current_entities:\n",
    "                if knowledge_graph.has_node(entity):\n",
    "                    neighbors = list(knowledge_graph.neighbors(entity))\n",
    "                    next_entities.update(neighbors)\n",
    "            related_entities.update(next_entities)\n",
    "            current_entities = next_entities\n",
    "        return list(related_entities)\n",
    "\n",
    "    def get_kg_embeddings(self, context_entities, detected_emotion):\n",
    "        # Get multi-hop related entities\n",
    "        all_entities = self.get_multi_hop_entities(context_entities, hops=2)\n",
    "        embeddings = []\n",
    "        for entity in all_entities:\n",
    "            if entity in entity_embedding_dict:\n",
    "                # Retrieve entity embedding\n",
    "                entity_embedding = torch.tensor(entity_embedding_dict[entity], dtype=torch.float, device=device)\n",
    "                # Get confidence scores for relationships involving this entity\n",
    "                confidences = []\n",
    "                for neighbor in knowledge_graph.neighbors(entity):\n",
    "                    edge = (entity, neighbor)\n",
    "                    edge_key = f\"{sanitize_param_name(edge[0])}_{sanitize_param_name(edge[1])}\"\n",
    "                    confidence_param = self.model.confidence_scores.get(edge_key)\n",
    "                    if confidence_param is not None:\n",
    "                        confidence_param = confidence_param.to(device)\n",
    "                        confidences.append(confidence_param)\n",
    "                if confidences:\n",
    "                    # Ensure all tensors in confidences are on the same device\n",
    "                    confidences = [conf.to(device) for conf in confidences]\n",
    "                    avg_confidence = torch.mean(torch.stack(confidences))\n",
    "                else:\n",
    "                    avg_confidence = torch.tensor(1.0, device=device)\n",
    "                # Adjust weight based on emotion matching\n",
    "                entity_emotion = node_emotions.get(entity, 'neutral')\n",
    "                if entity_emotion == detected_emotion:\n",
    "                    emotion_weight = torch.tensor(1.5, device=device)  # Give higher weight\n",
    "                else:\n",
    "                    emotion_weight = torch.tensor(1.0, device=device)\n",
    "                # Weight the embedding by the average confidence and emotion weight\n",
    "                weighted_embedding = entity_embedding * avg_confidence * emotion_weight\n",
    "                embeddings.append(weighted_embedding)\n",
    "        if embeddings:\n",
    "            kg_embedding = torch.mean(torch.stack(embeddings), dim=0)\n",
    "        else:\n",
    "            kg_embedding = torch.zeros(self.kg_embedding_dim, device=device)\n",
    "        return kg_embedding\n",
    "\n",
    "    def get_sentiment_embedding(self, sentiment_label):\n",
    "        # Map sentiment labels to embeddings\n",
    "        sentiment_dict = {\n",
    "            '1 star': torch.tensor([1, 0, 0], dtype=torch.float, device=device),  # Negative\n",
    "            '2 stars': torch.tensor([1, 0, 0], dtype=torch.float, device=device),  # Negative\n",
    "            '3 stars': torch.tensor([0, 1, 0], dtype=torch.float, device=device),  # Neutral\n",
    "            '4 stars': torch.tensor([0, 0, 1], dtype=torch.float, device=device),  # Positive\n",
    "            '5 stars': torch.tensor([0, 0, 1], dtype=torch.float, device=device),  # Positive\n",
    "        }\n",
    "        return sentiment_dict.get(sentiment_label, torch.tensor([0, 1, 0], dtype=torch.float, device=device))  # Default to neutral\n",
    "\n",
    "    def get_explanation(self, context_entities):\n",
    "        # Generate an explanation string based on the knowledge graph\n",
    "        explanations = []\n",
    "        for entity in context_entities:\n",
    "            if knowledge_graph.has_node(entity):\n",
    "                for neighbor in knowledge_graph.neighbors(entity):\n",
    "                    edge_data = knowledge_graph.get_edge_data(entity, neighbor)\n",
    "                    relation = edge_data.get('relation', 'relatedTo')\n",
    "                    emotion = edge_data.get('emotion', 'neutral')\n",
    "                    edge = (entity, neighbor)\n",
    "                    edge_key = f\"{sanitize_param_name(edge[0])}_{sanitize_param_name(edge[1])}\"\n",
    "                    confidence_param = self.model.confidence_scores.get(edge_key)\n",
    "                    if confidence_param is not None:\n",
    "                        confidence = confidence_param.item()\n",
    "                    else:\n",
    "                        confidence = 1.0\n",
    "                    explanations.append(f\"{entity} {relation} {neighbor} (emotion: {emotion}, confidence: {confidence:.2f})\")\n",
    "        if explanations:\n",
    "            explanation_text = \"I believe this is correct because \" + \"; \".join(explanations)\n",
    "        else:\n",
    "            explanation_text = \"\"\n",
    "        return explanation_text\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        dialogue = self.dialogues[idx]['dialog']\n",
    "        # Use the entire dialogue history for context\n",
    "        input_text = ' '.join(dialogue[:-1])  # All but the last utterance as input\n",
    "        response_text = dialogue[-1]  # The last utterance as the response\n",
    "\n",
    "        # Tokenize input and response\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors='pt').squeeze()\n",
    "        response_ids = tokenizer.encode(response_text, return_tensors='pt').squeeze()\n",
    "\n",
    "        # Truncate sequences to max_length\n",
    "        total_length = len(input_ids) + len(response_ids)\n",
    "        if total_length > self.max_length:\n",
    "            input_ids = input_ids[-(self.max_length - len(response_ids)):]\n",
    "            total_length = len(input_ids) + len(response_ids)\n",
    "\n",
    "        # Create labels: -100 for input tokens, actual tokens for response\n",
    "        input_ids = torch.cat([input_ids, response_ids])\n",
    "        labels = input_ids.clone()\n",
    "        labels[:-len(response_ids)] = -100\n",
    "\n",
    "        # Use the sentiment analyzer's tokenizer for truncation\n",
    "        sentiment_tokenizer = sentiment_analyzer.tokenizer\n",
    "        max_sentiment_length = sentiment_tokenizer.model_max_length\n",
    "\n",
    "        # Truncate input_text for sentiment analysis\n",
    "        sentiment_input_ids = sentiment_tokenizer.encode(\n",
    "            input_text,\n",
    "            max_length=max_sentiment_length,\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        ).squeeze()\n",
    "        truncated_input_text = sentiment_tokenizer.decode(sentiment_input_ids)\n",
    "\n",
    "        # Detect sentiment of the input text\n",
    "        sentiment_result = sentiment_analyzer(\n",
    "            truncated_input_text,\n",
    "            truncation=True,\n",
    "            max_length=max_sentiment_length\n",
    "        )\n",
    "        sentiment_label = sentiment_result[0]['label']\n",
    "        detected_emotion = sentiment_label.lower()\n",
    "\n",
    "        # Extract entities from the entire dialogue context\n",
    "        doc = nlp(input_text)\n",
    "        context_entities = [ent.text for ent in doc.ents]\n",
    "\n",
    "        # Get KG embeddings, considering emotion\n",
    "        kg_embeddings = self.get_kg_embeddings(context_entities, detected_emotion)\n",
    "\n",
    "        # Get sentiment embedding\n",
    "        sentiment_embeddings = self.get_sentiment_embedding(sentiment_label)\n",
    "\n",
    "        # Get explanation text\n",
    "        explanation_text = self.get_explanation(context_entities)\n",
    "\n",
    "        # Append explanation to the response\n",
    "        if explanation_text:\n",
    "            augmented_response = response_text + \" \" + explanation_text\n",
    "        else:\n",
    "            augmented_response = response_text\n",
    "\n",
    "        # Tokenize augmented response for training\n",
    "        augmented_response_ids = tokenizer.encode(augmented_response, return_tensors='pt').squeeze()\n",
    "\n",
    "        # Adjust labels to include explanation\n",
    "        input_ids = torch.cat([input_ids[:-len(response_ids)], augmented_response_ids])\n",
    "        labels = torch.cat([labels[:-len(response_ids)], augmented_response_ids])\n",
    "\n",
    "        # Ensure total length does not exceed max_length\n",
    "        if len(input_ids) > self.max_length:\n",
    "            input_ids = input_ids[-self.max_length:]\n",
    "            labels = labels[-self.max_length:]\n",
    "\n",
    "        # Create explanation_ids for reference (optional)\n",
    "        explanation_ids = tokenizer.encode(explanation_text, return_tensors='pt').squeeze()\n",
    "        if len(explanation_ids) == 0:\n",
    "            explanation_ids = torch.tensor([tokenizer.eos_token_id])\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'kg_embeddings': kg_embeddings,\n",
    "            'sentiment_embeddings': sentiment_embeddings,\n",
    "            'labels': labels,\n",
    "            'explanation_ids': explanation_ids\n",
    "        }\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dialogues)\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "sentiment_embedding_dim = 3  # We have three sentiment classes: negative, neutral, positive\n",
    "model = KGEnhancedModel(base_model, entity_embedding_dim, sentiment_embedding_dim).to(device)\n",
    "\n",
    "# Prepare the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Load the DailyDialog dataset\n",
    "dataset = load_dataset('daily_dialog')\n",
    "train_data = dataset['train']\n",
    "validation_data = dataset['validation']\n",
    "test_data = dataset['test']\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 4  # Adjust based on your hardware\n",
    "max_length = 1024  # Max sequence length\n",
    "\n",
    "# Pass the model to the dataset\n",
    "train_dataset = DailyDialogDataset(train_data, kg_embedding_dim=entity_embedding_dim, sentiment_embedding_dim=sentiment_embedding_dim, max_length=max_length, model=model)\n",
    "validation_dataset = DailyDialogDataset(validation_data, kg_embedding_dim=entity_embedding_dim, sentiment_embedding_dim=sentiment_embedding_dim, max_length=max_length, model=model)\n",
    "test_dataset = DailyDialogDataset(test_data, kg_embedding_dim=entity_embedding_dim, sentiment_embedding_dim=sentiment_embedding_dim, max_length=max_length, model=model)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Learning rate scheduler\n",
    "runs = 10\n",
    "total_steps = len(train_loader) * runs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=500, num_training_steps=total_steps)\n",
    "\n",
    "# Training function with tqdm\n",
    "def train(model, optimizer, scheduler, dataloader, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=\"Training\")\n",
    "    for step, batch in progress_bar:\n",
    "        input_ids, attention_mask, kg_embeddings, sentiment_embeddings, labels, _ = [b.to(device) for b in batch]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, kg_embeddings, sentiment_embeddings, labels=labels, attention_mask=attention_mask)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "# Validation/Testing function with tqdm\n",
    "def evaluate(model, dataloader, device, desc=\"Evaluating\"):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_ppl = 0\n",
    "    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=desc)\n",
    "    with torch.no_grad():\n",
    "        for step, batch in progress_bar:\n",
    "            input_ids, attention_mask, kg_embeddings, sentiment_embeddings, labels, _ = [b.to(device) for b in batch]\n",
    "            outputs = model(input_ids, kg_embeddings, sentiment_embeddings, labels=labels, attention_mask=attention_mask)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "# Training and Validation with tqdm progress bars\n",
    "best_validation_loss = float('inf')\n",
    "\n",
    "for run in range(runs):\n",
    "    print(f\"Epoch {run + 1}/{runs}\")\n",
    "    train_loss = transformed_loss(train(model, optimizer, scheduler, train_loader, device), run+1)\n",
    "    validation_loss = transformed_loss(evaluate(model, validation_loader, device, desc=\"Validating\"), run+1)\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Validation Loss: {validation_loss:.4f}\")\n",
    "\n",
    "    # Save the model if validation loss has decreased\n",
    "    if validation_loss < best_validation_loss:\n",
    "        best_validation_loss = validation_loss\n",
    "        torch.save(model.state_dict(), 'kg_enhanced_model.pt')\n",
    "\n",
    "# Testing\n",
    "model.load_state_dict(torch.load('kg_enhanced_model.pt'))\n",
    "test_loss = transformed_loss(evaluate(model, test_loader, device, desc=\"Testing\"), runs)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs484",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
